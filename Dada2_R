Metabarcoding analyses: 1) trim the primers using cutadapter, 2) analyse the Fastq data with Dada2, 3) assing the taxonomy of the generated ASVs using databases, 4) data analyses and plots using R. 

################################################################ 
################################################################ 
### 1) Cutadapter to trim primers

# Create directory "fastq_np" for the trimed sequences 
mkdir fastq_np
# Activate cutadap 
conda activate /path

# Use your set of FWD and REV primers (example below is amplicon of the region 16S V6-V8)
FWD_primer=ACGCGHNRAACCTTACC
REV_primer=ACGGGCRGTGWGTRCAA
DISS=0.2
for sample in $(cat samples)
do
    echo "On sample: $sample"
    cutadapt -g ^${FWD_primer} \
    -G ^${REV_primer} \
    -e $DISS \
    -j 20 \
    --discard-untrimmed \
    -o fastq_np/${sample}_L001_R1_001.fastq.gz -p fastq_np/${sample}_L001_R2_001.fastq.gz \
    ${sample}_L001_R1_001.fastq.gz ${sample}_L001_R2_001.fastq.gz \
    >> cutadapt_primer_trimming_stats.txt 2>&1
done

################################################################ 
################################################################ 
### 2) Dada2 

# Explore your data using different parameters and checking the quality using the R and Dada2 package. Once you decide on the parameters, you can also create a Rscript and a bash script to run the code.

# Create a R script
nano dada2.r

# File dada2.r:

library("dada2")
library("ShortRead")
library('ggplot2')
library('icesTAF')
setDadaOpt(DETECT_SINGLETONS = TRUE)
setwd("/flash/HusnikU/Duda/01_Dada2_18S_teste230731/fastq_np") 
mkdir('../4.07.2023') 
mkdir('../4.07.2023/vis') 
dir <- '4.07.2023/' 
list.files() 
forward_reads <- sort(list.files(pattern="_L001_R1_001.fastq.gz")) 
reverse_reads <- sort(list.files(pattern="_L001_R2_001.fastq.gz")) 
samples <- gsub('_L001_R1_001.fastq.gz','',forward_reads)
forward_reads_check <- as.data.frame(forward_reads)
forward_reads_check$aaa <- as.data.frame(samples)
forward_reads_check
# Create a figure for quality control of the reads (plotting for heavy datasets can take >10min)
filtered_forward_reads <- paste0("../",dir, 'filtered/', samples, "_R1_filtered.fastq")
filtered_reverse_reads <- paste0("../",dir, 'filtered/', samples, "_R2_filtered.fastq")
f_reads_quality <- plotQualityProfile(forward_reads)
r_reads_quality <- plotQualityProfile(reverse_reads)
pdf(paste0('../',dir,'vis/qulity_no_filt.pdf'), height = 250)
gridExtra::grid.arrange(f_reads_quality, r_reads_quality, ncol = 2)
dev.off()
# Check plots and filter low quality reads. Set the parameters according to your data (there are additional flags that can be use if needed: https://benjjneb.github.io/dada2/tutorial.html)
filtered_out <- filterAndTrim(forward_reads, filtered_forward_reads, 
                              reverse_reads, filtered_reverse_reads, 
                              compress = F,
                              truncQ = 0,
                              multithread=TRUE, 
                              maxEE = c(3,6))
filtered_out <- as.data.frame(filtered_out)
filtered_out[,3] <- filtered_out$reads.out*100/filtered_out$reads.in
write.table(filtered_out, paste0('../',dir,'/filtered_out.txt'), sep="\t", quote=F)
filtered_out 
filtered_forward_reads_quality_plot <- plotQualityProfile(filtered_forward_reads)
filtered_reverse_reads_quality_plot <- plotQualityProfile(filtered_reverse_reads)
pdf(paste0('../',dir,'vis/qulity_filtered.pdf'), height = 250)
gridExtra::grid.arrange(filtered_forward_reads_quality_plot, filtered_reverse_reads_quality_plot, ncol = 2)
dev.off()
#Generate an error model of our data
err_forward_reads <- learnErrors(filtered_forward_reads, nbases = 1e8, multithread=TRUE)
err_reverse_reads <- learnErrors(filtered_reverse_reads, nbases = 1e8, multithread=TRUE)
write.table(err_forward_reads, paste0('../',dir,'err_forward.txt'), sep="\t", quote=F)
write.table(err_reverse_reads, paste0('../',dir,'err_reverse.txt'), sep="\t", quote=F)
err_forward_reads_plot <-plotErrors(err_forward_reads, nominalQ=TRUE)
err_reverse_reads_plot <- plotErrors(err_reverse_reads, nominalQ=TRUE)
pdf(paste0('../',dir,'vis/f_err_qulity.pdf'))
err_forward_reads_plot
dev.off()
pdf(paste0('../',dir,'vis/r_err_qulity.pdf'))
err_reverse_reads_plot
dev.off()
#Dereplication
derep_forward <- derepFastq(filtered_forward_reads, verbose=TRUE)
names(derep_forward) <- samples 
derep_reverse <- derepFastq(filtered_reverse_reads, verbose=TRUE)
names(derep_reverse) <- samples
# Denoised amplicons 
dada_forward <- dada(derep_forward, err=err_forward_reads, multithread=TRUE, pool= TRUE)
dada_reverse <- dada(derep_reverse, err=err_reverse_reads, multithread=TRUE, pool= TRUE)
#Merge forward and reverse reads
merged_amplicons <- mergePairs(dada_forward, derep_forward, dada_reverse, derep_reverse, minOverlap = 18)
class(merged_amplicons)
length(merged_amplicons)
names(merged_amplicons) 
#Count table
seqtab <- makeSequenceTable(merged_amplicons)
write.table(seqtab, paste0('../',dir,'/seqtab.txt'), sep="\t", quote=F)
class(seqtab) 
dim(seqtab) 
pdf(paste0('../',dir,'vis/Distribution_of_sequence_lengths_before.pdf'))
hist(nchar(getSequences(seqtab)), main="Distribution of sequence lengths")
dev.off()
# Filter amplicons to appropriate size
seqtab_filtered <- seqtab[,nchar(colnames(seqtab)) %in% 450:600] 
pdf(paste0('../',dir,'vis/Distribution_of_sequence_lengths_after.pdf'))
hist(nchar(getSequences(seqtab_filtered)), main="Distribution of sequence lengths")
dev.off()
#Chimera identification
seqtab.nochim <- removeBimeraDenovo(seqtab_filtered, method = "pooled", multithread=T, verbose=T)
write.table(seqtab.nochim, paste0('../',dir,'/seqtab.nochim.txt'), sep="\t", quote=F)
sum(seqtab.nochim)/sum(seqtab_filtered)

# Create summarize table
getN <- function(x) sum(getUniques(x))
summary_tab <- data.frame(row.names=samples, dada2_input=filtered_out[,1], filtered=filtered_out[,2], dada_f=sapply(dada_forward, getN), dada_r=sapply(dada_reverse, getN), merged=sapply(merged_amplicons, getN), nonchim=rowSums(seqtab.nochim), final_perc_reads_retained=round(rowSums(seqtab.nochim)/filtered_out[,1]*100, 1))
summary_tab
write.table(summary_tab, paste0('../',dir,'/dada2_summary.txt'), sep="\t", quote=F)

# giving our headers more manageable names (ASV_1, ASV_2...)
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")
for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep="_")
}

# making and writing out a fasta of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, paste0('../',dir,'/ASVs.fa'))
# count table:
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.table(asv_tab, paste0('../',dir,'/ASVs_counts.txt'), sep="\t", quote=F)

########### END R script ###########

# Create a bash file 
nano bash_dada2.slurm 

# Bash file:

#!/bin/bash
#SBATCH --partition=compute
#SBATCH --time=60:00:00
#SBATCH --mem=500G
#SBATCH -c 40
#SBATCH -n 1
#SBATCH --job-name=dada2.r 
#SBATCH --mail-type=ALL
#SBATCH --mail-user=EMAIL_USER

ml load R/4.2.1
Rscript dada2script230731.r

################################################################ 
################################################################ 
### 3) Assigning taxonomy

# Create an "assign_sintax.sh" file
nano assign_sintax.sh 

# assign_sintax.sh file*

#!/usr/bin/env bash
mkdir parted_fasta/
cd parted_fasta/
split -l 300 ../$1
ls * > samples
for file in $(cat samples)
do
    echo "On file: $file"
/apps/usearch11 \
-sintax $file \
-db $2 \
-tabbedout combined_taxonomy_${file}.txt \
-strand both -sintax_cutoff 0.8
done
cat *.txt >> ../combined_taxonomy.txt
cd ../
rm -r parted_fasta/

# Run the assign_sintax.sh file with the ASVs and database
/path/assign_sintax.sh "FASTA_FILE" "DATABASE_PATH"

# Create "fix_sintax.sh" file

nano fix_sintax.sh

# fix_sintax.sh file
#!/usr/bin/env bash
awk -F"\t" '{print $1 "\t" $4}' < $1 | awk '{gsub(/,/,"\t", $0); print}' > taxonomy_fixed.txt

# Run the fix_sintax.sh with the output of the assign_sintax.sh (combined_taxonomy.txt file)
./fix_sintax.sh combined_taxonomy.txt 

# Add row in 'taxonomy_fixed.txt' with the label of the taxon level
tax_part <- read.csv('taxonomy_fixed.txt', header = T, sep = '\t')
count_part <- read.csv('ASVs_counts.txt', header = T, sep = '\t')
rownames(tax_part) <- tax_part[,1]
tax_part <- tax_part[,-1]
merged_parts <- merge(count_part, tax_part, by = 0)
write.table(merged_parts, 'full_table.txt', sep = "\t", quote = F, row.names = F)

################################################################ 
################################################################ 
### 4) Data analyses R

# Dada2 Visualization 16S
library("phyloseq")
library('dplyr')
library('ggplot2')
library('gridExtra')
library('ape')
library('vegan')
library('FSA')
library('plyr')
library('dunn.test')
library('ggsci')
library('EcolUtils')
library('cluster')
library(zoo)
library(ampvis2)
library("viridis") 
library(ggpmisc)
library(MASS)
library(ggcorrplot)
library(ggvenn)
library(VennDiagram)
library(ggpubr)

setwd('/PATH')
# Read all data
mapping_file <- read.table('mapping_Duda.txt', sep = '\t', dec = ',', quote = "", header = T)
tax_part <- read.csv('taxonomy_fixed.txt', header = T, sep = '\t')
count_part <- read.csv('ASVs_counts.txt', header = T, sep = '\t')
rownames(count_part) <- count_part[,1]
count_part <- count_part[,-1]
rownames(tax_part) <- tax_part[,1]
tax_part <- tax_part[,-1]
#by = 0 row means the labels
merged_parts <- merge(count_part, tax_part, by = 0)
write.table(merged_parts, 'full_table.txt', sep = "\t", quote = F, row.names = F)

full_table <- read.table('full_table.txt', sep = '\t', header = T,comment.char  = '')
rownames(full_table) <- full_table$Row.names
full_table <- subset(full_table, full_table$Domain != '')
full_table <- subset(full_table, full_table$Genus != 'g:Cutibacterium')
full_table <- subset(full_table, full_table$Class != 'c:Chloroplast')
full_table <- subset(full_table, full_table$Family != 'f:Yersiniaceae')
full_table[full_table == ''] <- NA
full_table <- data.frame(t(na.locf(t(full_table))))
full_table[,2:(ncol(full_table) - 6)] <- apply(full_table[,2:(ncol(full_table) - 6)], 2, as.numeric)
full_table <- subset(full_table, apply(full_table[,2:(ncol(full_table) - 6)] , 1, sum) > 3)
 
# Fix taxonomy
tax_levels <- c('Domain', 'Phylum', 'Class','Order', 'Family', 'Genus')
patterns <- c('(k:.*)|(.*_X)|(.*_X_)|(.*_XX)|(.*_XX_)|(.*_XXX)|(.*_XXX_)|(.*_XXXX)|(.*_XXXX_)|(*.-[:alpha:])|(*.-[:alpha:].*)', 
              '(k:.*)|(d:.*)|(.*_X)|(.*_X_)|(.*_XX)|(.*_XX_)|(.*_XXX)|(.*_XXX_)|(.*_XXXX)|(.*_XXXX_)|(*.-[:alpha:])|(*.-[:alpha:].*)',
              '(k:.*)|(d:.*)|(p:.*)|(.*_X)|(.*_X_)|(.*_XX)|(.*_XX_)|(.*_XXX)|(.*_XXX_)|(.*_XXXX)|(.*_XXXX_)|(*.-[:alpha:])|(*.-[:alpha:].*)',
              '(k:.*)|(d:.*)|(p:.*)|(c:.*)|(.*_X)|(.*_X_)|(.*_XX)|(.*_XX_)|(.*_XXX)|(.*_XXX_)|(.*_XXXX)|(.*_XXXX_)|(.*-[A-z])|(.*-[A-z].*)', 
              '(k:.*)|(d:.*)|(p:.*)|(c:.*)|(o:.*)|(.*_X)|(.*_X_)|(.*_XX)|(.*_XX_)|(.*_XXX)|(.*_XXX_)|(.*_XXXX)|(.*_XXXX_)|(.*-[A-z])|(.*-[A-z].*)',
              '(k:.*)|(d:.*)|(p:.*)|(c:.*)|(o:.*)|(f:.*)|(.*_X)|(.*_X_)|(.*_XX)|(.*_XX_)|(.*_XXX)|(.*_XXX_)|(.*_XXXX)|(.*_XXXX_)|(.*-[A-z])|(.*-[A-z].*)',
              '(k:.*)|(d:.*)|(p:.*)|(c:.*)|(o:.*)|(f:.*)|(g:.*)|(.*_X)|(.*_X_.*)|(.*_XX)|(.*_XX_.*)|(.*_XXX)|(.*_XXX_.*)|(.*_XXXX)|(.*_XXXX_.*)|(.*_XXXXX)|(.*_XXXXX.*)|(.*_XXXXX )|(.*-[A-z])|(.*-[A-z].*)|(.*_sp.)')

full_table_fixed <- full_table
count_var <- 1
for (fix_name in tax_levels) {
  full_table_fixed[,fix_name] <- gsub(patterns[count_var],'Unclassified',full_table_fixed[,fix_name])
  count_var <- count_var + 1
}
full_table_fixed$Genus <- gsub('UnclassifiedX','Unclassified',full_table_fixed$Genus)
full_table <- full_table_fixed

write.table(full_table, 'full_table_fixed_tax.tsv', col.names = T, sep = '\t', row.names = F)
write.table(full_table, 'full_table_for_ready_for_analysis.tsv', col.names = T, sep = '\t', row.names = F)

for (z in tax_levels) {
  aggregated_full_table <- aggregate(full_table[,2:(ncol(full_table) - 6)], by = list(full_table[,z]), FUN = sum)
  aggregated_full_table_with_procents <- as.data.frame(prop.table(as.matrix(aggregated_full_table[,-1]),2)*100)
  aggregated_full_table_with_procents[,z] <- aggregated_full_table[,1]
  write.table(aggregated_full_table ,paste0('finish_vis/counts_and_percents/counts_sums_per_each_',z,'.txt'), sep = "\t", quote = F, row.names = F)
  write.table(aggregated_full_table_with_procents ,paste0('finish_vis/counts_and_percents/percents_per_each_',z,'.txt'), sep = "\t", quote = F, row.names = F)
}

count_table <- full_table[,2:(ncol(full_table) - 6)]
row.names(count_table) <- full_table[,1]
colnames(count_table) <- mapping_file[,1]

tax_table <- full_table[,(ncol(full_table) - 6):(ncol(full_table))]
row.names(tax_table) <- full_table[,1]
tax_table <- as.matrix(tax_table)

rownames(mapping_file) <- mapping_file[,1]
OTU = otu_table(count_table, taxa_are_rows = T)
TAX = tax_table(tax_table)
samples =sample_data(mapping_file)
physeq = phyloseq(OTU, TAX,samples)

#alpha diversity calculation
pdf('finish_vis/rarecurve_plot_3ton.pdf')
rarecurve(t(count_table), step=25, cex=0.2)
dev.off()

ds <- amp_load(
  count_table,
  metadata = NULL,
  taxonomy = tax_table,
  fasta = NULL,
  tree = NULL,
  pruneSingletons = FALSE
)

pdf('finish_vis/octave_plot.pdf', width = 15, height = 15)
amp_octave(ds,
           scales = "free_y",
           num_threads = 8
)
dev.off()

# Unique ASVs
count_table_unique <- ifelse(count_table >= 1, 1,0)
check <- data.frame(apply(count_table_unique, 1, sum))
unique_asvs <- rownames(subset(check, check$apply.count_table_unique..1..sum. == 1))
count_table_unique <- subset(count_table_unique, rownames(count_table_unique) %in% unique_asvs)
count_table_unique_pie <- data.frame(apply(count_table_unique, 1, sum))
 
count_table_unique_pie_merge <- merge(count_table_unique_pie, full_table[,119:124], by = 0, all = F)
count_table_unique <- data.frame(apply(count_table_unique, 2, sum))
count_table_unique <- cbind(count_table_unique, mapping_file)

for (i in tax_levels) {
  
  for_plot_unique_taxa <- aggregate(count_table_unique_pie_merge$apply.count_table_unique..1..sum., list(count_table_unique_pie_merge[,i]), sum)
  colnames(for_plot_unique_taxa) <- c('Taxa', 'ASVs_count')
  for_plot_unique_taxa <- for_plot_unique_taxa[order(for_plot_unique_taxa$ASVs_count),]
  for_plot_unique_taxa <- 
    
    bar_plot_unique <- ggplot(for_plot_unique_taxa, aes(x=ASVs_count, y=Taxa)) +
    geom_bar(stat="identity") + scale_y_discrete(limits = for_plot_unique_taxa$Taxa)
  
  ggsave(
    paste0('finish_vis/unique/dist_of_unique_barplot',i,'.pdf'),
    plot = bar_plot_unique,
    path = NULL,
    width = 9.5,
    height = 15,
    scale = 1,
    dpi = 300,
    limitsize = F
  )  
}

geom_point_unique <- ggplot(count_table_unique, aes(x = factor(count_table_unique$Sample), y = count_table_unique[,1], colour = Ocean)) + 
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 

ggsave(
  paste0('finish_vis/unique/dist_of_unique_point.pdf'),
  plot = geom_point_unique,
  device = NULL,
  path = NULL,
  scale = 1,
  dpi = 300,
  limitsize = F
)

########################################################################
# Alpha diversity
alpha_div <- estimate_richness(physeq, split = TRUE, measures = c("Shannon"))
alpha_div_with_mapping <- cbind(alpha_div, mapping_file)
alpha_div_with_mapping$Sample <- as.character(alpha_div_with_mapping$Sample)

pdf('finish_vis/alpha_div_plot.pdf')
plot_richness(physeq, measures=c("Shannon")) + scale_x_discrete(limits=mapping_file$Sample)
dev.off()

# Alpha div indexes and normality test
# mkdir alpha_div
# cd alpha_div
# mkdir normality_check

a_index <- c("Shannon")

for (k in a_index) {
  alpha_div_cycle_plot <- plot_richness(physeq, measures=k) +
    scale_x_discrete(limits=mapping_file$Sample) +
    ggtitle(k) + theme(plot.title = element_text(hjust = 0.5))
  
  ggsave(
    paste0('finish_vis/alpha_div/',k,'.pdf'),
    plot = alpha_div_cycle_plot,
    device = NULL,
    path = NULL,
    width = 15,
    height = 7,
    scale = 1,
    dpi = 300,
    limitsize = F
  )
  
  pdf(paste0('finish_vis/alpha_div/normality_check/',k,'3ton.pdf'))
  hist(alpha_div_with_mapping[,k])
  dev.off()
}

# Test distribution
shapiro.test(alpha_div_with_mapping$Shannon)

########################################################################
# Boxplots of alpha diversity indexes
# mkdir boxplot
# cd boxplot 
# mkdir by_phylum

alpha_div_by_phylum <- data.frame()
for (i in unique(full_table_fixed$Phylum)) {
  alpha_div_by_phylum_inter_var <- estimate_richness(subset_taxa(physeq, Phylum==i), split = TRUE, measures = c("Shannon"))
  alpha_div_by_phylum_inter_var$Phylum <- i
  alpha_div_by_phylum_inter_var <- cbind(alpha_div_by_phylum_inter_var, mapping_file)
  alpha_div_by_phylum <- rbind(alpha_div_by_phylum,alpha_div_by_phylum_inter_var)
  alpha_div_by_phylum_inter_var <- NULL
}

alpha_div_by_phylum$Phylum <- gsub('d:','', alpha_div_by_phylum$Phylum)

my_cols <- c("dodgerblue3", "deeppink3","darkblue", "darkgoldenrod1", "darkseagreen", "darkorchid", "darkolivegreen1", "red", "darkgreen", "lightskyblue", "deeppink", "gray76", "firebrick", "brown1", "darkorange1", "cyan1", "royalblue4", "darksalmon", "darkblue",
             "royalblue4", "dodgerblue3", "steelblue1", "lightskyblue", "darkseagreen", "darkgoldenrod1", "darkseagreen", "darkorchid", "darkolivegreen1", "brown1", "darkorange1", "cyan1", "darkgrey")

for (i in c("Shannon")) {
  
  boxplot_by_phylum <- ggplot(alpha_div_by_phylum, aes(x = Phylum,y = alpha_div_by_phylum[,i], fill = Ocean)) +
    geom_boxplot() +
    theme(text = element_text(size=20)) + 
    labs(y = i) + 
    theme_bw() + 
    theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
    scale_fill_manual(values=my_cols) +
    theme(legend.key.size = unit(2, 'cm'), #change legend key size
          legend.key.height = unit(2, 'cm'), #change legend key height
          legend.key.width = unit(2, 'cm'), #change legend key width
          legend.title = element_text(size=20), #change legend title font size
          legend.text = element_text(size=16))
  
  ggsave(
    paste0('finish_vis/alpha_div/boxplot/by_phylum/',i,'3ton.pdf'),
    plot = boxplot_by_phylum,
    device = NULL,
    path = NULL,
    width = 21,
    height = 5,
    scale = 1,
    dpi = 300,
    limitsize = F
  )
  
}

write.table(alpha_div_by_phylum,'alpha_div_byphylum.txt', sep = "\t", quote = F, row.names = F)

# dunnTest
result <- data.frame()
for (phylum_var in unique(alpha_div_by_phylum$Phylum)) {
  phylum_to_check <- subset(alpha_div_by_phylum, alpha_div_by_phylum$Phylum == phylum_var)
  result <- rbind(result,c(phylum_var,kruskal.test(Shannon ~ Ocean,data = phylum_to_check)$p.value))
}
colnames(result) <- c('phylum', 'p.val')
write.table(result, 'Dunntest_phylum_result.txt', sep = "\t", quote = F, row.names = F)

########################################################################
# by full data
for (boxplot_cycle in a_index) {

  # Sites
  boxplot_site <- ggboxplot(alpha_div_with_mapping, x = "Region", y = boxplot_cycle,
                                   color = "Ocean", palette = my_cols,
                                   add = "jitter") + 
    stat_compare_means(aes(group = Region), label = "p.format", method = 'kruskal.test') 
  
  ggsave(
    paste0('finish_vis/alpha_div/boxplot/',boxplot_cycle,'_site3ton.pdf'),
    plot = boxplot_site,
    device = NULL,
    path = NULL,
    width = 12,
    height = 5,
    scale = 1,
    dpi = 300,
    limitsize = F
  )
}

# dunnTest
result_region <- dunnTest(alpha_div_with_mapping$Shannon ~ alpha_div_with_mapping$Region,
                          data=alpha_div_with_mapping,
                          method="bh", kw=TRUE)

capture.output(result_region, file = 'result_region.prn', sep = "/t", quote = F, row.names = F)

# Oceans
for (boxplot_cycle in a_index) {
  
  boxplot_ocean <- ggboxplot(alpha_div_with_mapping, x = "Ocean", y = boxplot_cycle,
                           color = "Ocean", palette = my_cols,
                           add = "jitter") + 
  stat_compare_means(aes(group = Ocean), label = "p.format", method = 'kruskal.test') 

ggsave(
  paste0('finish_vis/alpha_div/boxplot/',boxplot_cycle,'_ocean3ton.pdf'),
  plot = boxplot_ocean,
  device = NULL,
  path = NULL,
  width = 3,
  height = 5,
  scale = 1,
  dpi = 300,
  limitsize = F
)
}

########################################################################
# Rarefaction

taxa_names(physeq)
sample_sums(physeq)
hist(sample_sums(physeq))
min(sample_sums(physeq))
rarefied_otu_table <- rarefy_even_depth(physeq, sample.size = min(sample_sums(physeq)),
                                        rngseed = T, replace = TRUE, trimOTUs = TRUE, verbose = TRUE)

# Phyloseq object with percents instead of counts
physeq_percents <- transform_sample_counts(physeq, function(x) x / sum(x))

i <- NULL
for (i in c('Phylum')) {
  
# Gloome 
physeq_percents_Glommed = tax_glom(physeq_percents, i)
  
  
# Percents
plot_bar_percents <- plot_bar(physeq_percents_Glommed, fill=i) + 
    geom_bar(stat="identity")+
    theme(legend.text=element_text(size=15), legend.title=element_text(size=10)) +
    scale_x_discrete(limits=mapping_file$Sample) +
    scale_fill_manual(values = c("#e41a1c", "#377eb8", "#8dd3c7", "#ff7f00", "#984ea3", "#ffeda0", "#a65628", "#f781bf",
                                 "#999999", "#66c2a5", "#41ae76", "#fc8d62", "#e78ac3", "#bc80bd", "#ffd92f", "#e5c494",
                                 "#33a02c", "#8dd3c7", "#bebada", "#fb8072", "#80b1d3", "#fdb462", "#d9ef8b", "#fccde5",
                                 "#d9d9d9", "#a6d854", "#ccebc5", "#ffed6f", "#6a3d9a", "#b3b3b3", "#1f78b4", "#b15928"))
  
  ggsave(
    paste0('finish_vis/taxanomy/taxonomy_percents_',i,'3ton.pdf'),
    plot = plot_bar_percents,
    device = NULL,
    path = NULL,
    scale = 1,
    width = 25,
    #units = c("in", "cm", "mm"),
    dpi = 300,
    limitsize = F
  )
}

########################################################################
# Plot and analyse beta-div

my_cols <- c("#e41a1c", "#377eb8", "#e41a1c", "#0CBD66", "#984ea3", "#ffeda0", "#a65628", "#f781bf",
             "#999999", "#66c2a5", "#fc8d62", "#e78ac3", "#bc80bd", "#ffd92f", "#e5c494",
             "#33a02c", "#8dd3c7", "#fb8072", "#80b1d3", "#fdb462", "#d9ef8b", "#fccde5",
             "#d9d9d9", "#a6d854", "#ccebc5", "#ffed6f", "#6a3d9a", "#b3b3b3", "#1f78b4", "#b15928")

dist <- 'bray'
dist_methods <- c('bray')

for (dist in dist_methods) {
  dist_mx <- vegdist(t(as.data.frame(otu_table(rarefied_otu_table))), method= dist)
  ord <- metaMDS(dist_mx, k = 2)
  (fit <- envfit(ord, mapping_file[,14], perm = 999, na.rm = T))
  capture.output(fit, 
                 file = paste0('finish_vis/beta_div/vectors_',dist,'3ton.txt'))
  
  new_fit <-as.data.frame(cbind(scores(fit, display = "vectors"), 'r^2' = fit$vectors$r, 'p.value' = fit$vectors$pvals))
  new_fit$Species <- "T"  
  df <- data.frame(ord$points)
  colnames(df) <- c('NMDS1', 'NMDS2')
  df <- cbind(df, mapping_file)
 
  beta_div <- ggplot(df, aes(x = NMDS1, y = NMDS2))+
    geom_point(aes(color = Region, shape = Ocean), size = 3) + 
    geom_segment(data = new_fit,
                 aes(x = 0, xend = NMDS1, y = 0, yend = NMDS2),
                 arrow = arrow(length = unit(0.5, "cm")), colour = ifelse(new_fit$p.value < 0.05, 'red', 'black')) +
    geom_label(data = new_fit, aes(x = NMDS1, y = NMDS2, label = Species),
               size = 4, colour = "black", fontface = "bold") + 
    theme_bw() + 
    scale_color_manual(values=my_cols)
  
  ggsave(
    paste0('finish_vis/beta_div/NMDS_vec_',dist,'teste2_3ton.pdf'),
    plot = beta_div,
    device = NULL,
    path = NULL,
    scale = 1,
    dpi = 300,
    limitsize = F
  )
  
}

dist_mx_adonis <- vegdist(t(as.data.frame(otu_table(rarefied_otu_table))), method= 'bray')
dist_mx_adonis <- as.matrix(dist_mx_adonis)
dist_mx_adonis <- as.data.frame(dist_mx_adonis)
dist_mx_with_mapping <- data.frame()
dist_mx_with_mapping <- cbind(dist_mx_adonis, mapping_file)

# Adonis
adonis = adonis2(formula=dist_mx_with_mapping[,1:117] ~ Ocean + Region, data=dist_mx_with_mapping, permutations=999)
adonis_T = adonis2(formula=dist_mx_with_mapping[,1:117] ~ as.numeric(Temp) + Ocean + Region, data=dist_mx_with_mapping, permutations=999)

# Export 
capture.output(adonis,
               file = 'finish_vis/beta_div/adonis.txt')
capture.output(adonis_T,
               file = 'finish_vis/beta_div/adonis_T.txt')

# Pairwise
library(pairwiseAdonis)
### Make factors out of columns.
dist_mx_with_mapping$Ocean = as.factor(dist_mx_with_mapping$Ocean)
dist_mx_with_mapping$Region = as.factor(dist_mx_with_mapping$Region)
### one factor to analyze differences.
##ocean
pw_adonis_ocean = pairwise.adonis2(dist_mx ~ Ocean, data = dist_mx_with_mapping,
                             permutations = 999,
                             method = 'bray',
                             padj = 'bonferroni')

capture.output(pairwise.adonis2(dist_mx ~ Ocean, data = dist_mx_with_mapping,
                                permutations = 999), 
               file = 'finish_vis/beta_div/pw_adonis_test_ocean.txt')

# Region
pw_adonis_region = pairwise.adonis2(dist_mx ~ Region, data = dist_mx_with_mapping,
                             permutations = 999,
                             method = 'bray',
                             padj = 'bonferroni')

capture.output(pairwise.adonis2(dist_mx ~ Region, data = dist_mx_with_mapping,
                                permutations = 999), 
               file = 'finish_vis/beta_div/pw_adonis_test_region.txt')
